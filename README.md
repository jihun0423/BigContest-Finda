# 핀다 앱 사용성 데이터를 통한 대출 여부 예측 
빅콘테스트 퓨처스리그
https://www.bigcontest.or.kr/points/content.php#ct04


<br />



# 1. 참여 계기  
빅콘테스트에서 개최한 데이터분석 퓨처스리그의 핀다앱 사용성 데이터를 통한 대출 여부 예측을 팀 프로젝트로 하기로 하였다.  
최종 정리 및 발표 자료는 pdf 파일에 저장하였다.


<br />




# 2. 참여 인원 / 기간
* 팀 프로젝트 (4인)
* 2022년 9월 1일 ~ 2023년 12월 1일


<br />




# 3. 사용 기술
* Python
* SQL
* Matplotlib
* Sklearn


<br />



# 4. 작업 환경
* Jupyter Notebook (CPU)
* Google Colab (GPU)


<br />



# 5. 프로젝트 진행 과정
<br />



## 1. 데이터 확인 및 전처리
* 데이터 셋은 총 2가지가 있는데, 핀다 앱을 통해 신청한 대출 신청 여부에 대한 데이터 Loan_result와, 가명화된 핀다 앱 사용자의 개인정보 User_spec이 있다.
* 타겟의 분포를 확인하였더니, 대출 여부 (is_applied) 불균형이 매우 심한걸로 나타나 불균형을 어떻게 처리하는지가 관건일 것으로 보였다.
* 각 데이터 셋에 대하여 결측치 처리를 해 주었다. 대체할 수 있는 (유추가 가능한) 결측치는 대체를 해 주었고, 유추가 불가능한 결측치는 제거를 해 주었다.
* 돈과 관련된 컬럼들은 분포가 매우 치우쳐 있어 머신러닝의 효과를 높이기 위하여 정규분포에 최대한 가깝게 되도록 변환을 해주었다.
* 전처리를 완료한 2가지 데이터 셋을 합쳐 하나의 데이터 셋으로 만들었다.


* 전처리 코드는 preprocessing.ipynb 


<br />


## 2. EDA 및 불필요한 컬럼 제거
* 범주형, 연속형 변수들을 타겟 컬럼과 연관지어 시각화 및 상관관계를 알아보았다. 대부분의 컬럼들이 타겟과의 상관관계가 거의 없다고 나왔다. 타겟의 불균형이 매우 심해서 그렇다고 생각한다.
* 이번 프로젝트의 EDA에서 가장 중요한 점은 어떤 컬럼을 머신러닝에 사용할지 정하는 것이였다. EDA.ipynb 파일의 맨 뒤에 기술 하였듯이, 이번 데이터들에서는 신용할 수 없는 데이터들이 굉장히 많았다. 이러한 변수들을 제거하여 신용할 수 있는 변수들만 남겨두었다.


* EDA 및 불필요한 컬럼 제거.ipynb 참조


<br />


## 3. 표준화 및 PCA
* 머신러닝을 통하여 학습할 때, 컬럼별로 숫자의 범위가 다를 경우 학습할 때 지장이 있을 수 있다. 따라서, 모든 연속형 변수에 대하여 Min-Max Scaler를 통해 0~1 범위로 맞추어주었다.
* PCA는 차원 축소를 통하여 데이터를 더 낮은 차원의 데이터로 정보를 함축시킨다. 머신러닝에 있어서 컬럼 수가 많을 수록, 성능이 저하될 수 있기에 차원 축소를 해 주었다.


* 표준화 및 pca.ipynb 참조


## 4. 불균형 해소
* 앞서 언급했듯이, 타겟 데이터의 불균형이 매우 심각하다 (93 : 7). 따라서, 불균형을 해소하기 위해 여러 방법을 사용하였다.  


1. 언더샘플링
* 데이터 수가 많은 라벨에 대하여 데이터 수를 줄여 균형을 맞추는 방법이다.  
이번 데이터 셋에는 800만개가 넘는 데이터가 있으므로, 언더샘플링을 해도 데이터 수는 충분히 많을 수도 있다고 생각해 진행을 해 보았다.   
하지만, 언더샘플링은 다수 데이터를 대부분 쓰지 않는 다는 것과 같으므로 데이터의 손실이 일어나 되려 성능이 안 좋아 질수도 있다.


2. 오버 샘플링
* 데이터 수가 적은 라벨에 대하여 데이터 수를 늘려 균형을 맞추는 방법이다.  
오버샘플링은 데이터 수가 적은 라벨의 데이터들을 최근접 이웃을 사용하여 새로운 데이터를 생성해 데이터 수를 늘린다.  
오버샘플링을 사용할 경우 데이터 수가 기존의 거의 2배가 되므로 더욱 더 정교한 학습이 가능해진다.  
하지만, 적은 수의 데이터를 이용하여 새로 데이터를 만든 것이므로, 이 적은 수의 데이터의 범주에 과적합이 될 가능성이 있다. 또한, 데이터 수가 굉장히 많아지므로 학습하는데 시간도 매우 오래 걸린다.

* 위의 2가지 방법과 불균형을 해소하지 않은 채 원본 데이터 그대로, 총 3가지의 방법으로 기본적인 머신러닝 모델을 돌려본 결과, 오버샘플링이 가장 성능이 좋게 나왔다.  
따라서, 오버샘플링으로 진행하기로 결정하고, 다양한 오버샘플링 기법들을 이용하여 가장 좋은 결과를 낸 오버샘플링 기법을 채용하였다. (Smotetomek)


* 오버샘플링,언더샘플링.ipynb 참조


<br />


## 4. 모델 설정
* 모델은 베이스라인에서 주어져 있던 efficientnetb0를 사용하였다.

<details>
<summary>모델 구조</summary>
 
![efficientnetb0](https://user-images.githubusercontent.com/131629615/235849301-1c2c6ee0-db99-4419-bd4c-671d940a7c9b.png)

</details>


<br />


## 5. 사용한 기법
* 앞서 라벨별로 증강한 이미지를 원본 데이터와 합친 뒤, Stratified K-Fold를 사용하여 라벨별로 균등하게 5개의 Fold로 나누어 교차검증을 진행하였다.
* 손실함수로는 CrossEntropyLoss에 Weight를 부여하여 사용하였다.
* 최적화를 위한 옵티마이저는 이미지 분류 문제에 흔히 쓰이는 Adam을 이용하였다.
* 모델 학습 속도 증가를 위해 AMP와 GradScaler를 사용하였다.


<br />




# 6. 결과
* 평가 기준은 WeightedF1Score로, Baseline에서 주어진 스코어는 0.2201 이였다.
* 내가 처음으로 나온 스코어는 0.2645로, Baseline보다는 높지만 그래도 여전히 너무 낮은 결과였다.


<br />




# 7. 회고 / 느낀점
# 파일 순서
preprocessing -> EDA -> 표준화,PCA -> 오버샘플링 -> 모델링 -> 앙상블
